{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1HcQmklTrO9gFNqX4AT7ciLAvl8zSBi8O","authorship_tag":"ABX9TyMX5mrgl88zeCCGohAKmVLt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["pip install fire"],"metadata":{"id":"4SpQ8d52THL4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673737519698,"user_tz":300,"elapsed":8222,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"0dd84efe-c696-43dc-a016-8231ea56dbf4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fire\n","  Downloading fire-0.5.0.tar.gz (88 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fire) (1.15.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire) (2.2.0)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=88eb11f30291c31e9adef21a70016236819539e4709332d3d5f6595b0511aebe\n","  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n","Successfully built fire\n","Installing collected packages: fire\n","Successfully installed fire-0.5.0\n"]}]},{"cell_type":"code","source":["pip install transformers"],"metadata":{"id":"WbNUkKVDTNH5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673737533921,"user_tz":300,"elapsed":14228,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"58fef1d8-7b39-44c6-d870-553a6607ecb7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"n_5Wxx-bBFXy","executionInfo":{"status":"ok","timestamp":1673737539805,"user_tz":300,"elapsed":5891,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"outputs":[],"source":["###############################\n","##### importing libraries #####\n","###############################\n","import json \n","import os\n","import random\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data.dataset import Dataset   \n","torch.backends.cudnn.benchmark=True\n","\n","import pyarrow.parquet as pq\n","import pandas as pd\n","import random\n","import fire\n","import logging\n","import os\n","import csv\n","\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm, trange\n","import torch.nn.functional as F\n","import math\n","from statistics import mean\n","import matplotlib.pyplot as plt\n","import pickle"]},{"cell_type":"code","source":["##### Hyperparameters for federated learning #########\n","num_clients = 20\n","num_selected = 5\n","num_rounds = 100\n","epochs = 5\n","batch_size = 16\n","datapath='/content/drive/MyDrive/Reserach_Notebooks/data/'\n","model_name=\"gpt2\"\n","tokenizer_name=\"gpt2\"\n","device= 'cuda'\n","epochs=5\n","lr=0.001\n","warmup_steps=5000\n","max_seq_len=128\n","train_count=num_clients*1000\n","output_dir=\"\"\n","save_model_on_epoch=False\n","ckpt_path=\"/content/drive/MyDrive/Reserach_Notebooks/ckpt/\"\n","misc_path=\"/content/drive/MyDrive/Reserach_Notebooks/misc/\"\n","#np.random.seed(112)\n","temp=0.5\n","ctrl_code=\"<|startoftext|>\""],"metadata":{"id":"t9L6Q0DmJUk7","executionInfo":{"status":"ok","timestamp":1673747140489,"user_tz":300,"elapsed":462,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":89,"outputs":[]},{"cell_type":"code","source":["\n","class DataComposer(torch.utils.data.Dataset):\n","    \n","    def __init__(self, control_code,texts, truncate=False, max_length=768):\n","\n","        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","        self.texts = []\n","        \n","        \n","        for row in texts:\n","            self.texts.append(torch.tensor(\n","                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n","            ))\n","                \n","        if truncate:\n","            self.texts = self.texts[:train_count]\n","        self.text_count = len(self.texts)\n","        \n","    def __len__(self):\n","        return self.text_count\n","\n","    def __getitem__(self, item):\n","        return self.texts[item]"],"metadata":{"id":"hWphcoVyVb2e","executionInfo":{"status":"ok","timestamp":1673747183409,"user_tz":300,"elapsed":1093,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":["#Pack Tensors"],"metadata":{"id":"DRdsVMBdY4lp"}},{"cell_type":"code","source":["def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n","    if packed_tensor is None:\n","        return new_tensor, True, None\n","    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n","        return packed_tensor, False, new_tensor\n","    else:\n","        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n","        return packed_tensor, True, None"],"metadata":{"id":"nMcH2gIOVec4","executionInfo":{"status":"ok","timestamp":1673743572515,"user_tz":300,"elapsed":865,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["#Read Data"],"metadata":{"id":"g-R8hmRx1fTh"}},{"cell_type":"code","source":["#Dataset specific tasks\n","with open(datapath+'imdb/all_texts.txt') as file:\n","  all_texts=file.read()\n","\n","all_words=all_texts.split()\n","text_raw=[]\n","frm=0\n","to=max_seq_len\n","for i in range(train_count):\n","  temp=' '.join(all_words[frm:to])\n","  text_raw.append(temp)\n","  frm+=max_seq_len\n","  to+=max_seq_len"],"metadata":{"id":"U9ZBRuotnNWS","executionInfo":{"status":"ok","timestamp":1673747160615,"user_tz":300,"elapsed":1660,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["# Dividing the training data into num_clients, with each client having equal number of images\n","def make_client_dataset(text_raw):\n","\n","  splits= torch.utils.data.random_split(text_raw, [int(len(text_raw) / num_clients) for _ in range(num_clients)])\n","  client_data=[]\n","  for s in splits:\n","    temp=[]\n","    for t in s:\n","      temp+=[t]\n","    client_data+=[temp]\n","  \n","  return client_data\n","\n","traindata_split = make_client_dataset(text_raw)"],"metadata":{"id":"s8iR7heKhoZm","executionInfo":{"status":"ok","timestamp":1673747163724,"user_tz":300,"elapsed":2,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["# Creating a pytorch loader for a Deep Learning model\n","train_loaders = [torch.utils.data.DataLoader(DataComposer(ctrl_code, x, truncate=True), batch_size=1, shuffle=True) for x in traindata_split]"],"metadata":{"id":"xQAKJoevolpS","executionInfo":{"status":"ok","timestamp":1673747236908,"user_tz":300,"elapsed":49921,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["def client_update(client_model, optimizer, train_dataloader, epoch=5):\n","    \"\"\"\n","    This function updates/trains client model on client data\n","    \"\"\"\n","    client_model = client_model.to(device)\n","    client_model.train()\n","\n","    # scheduler = get_linear_schedule_with_warmup(\n","    #     optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n","    # )\n","\n","    #accumulating_batch_count = 0\n","    input_tensor = None\n","    \n","    for epoch in range(epochs):\n","\n","        print(f\"Training epoch {epoch}\")\n","        for idx, entry in enumerate(train_dataloader):\n","            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, max_seq_len)\n","\n","            if carry_on and idx != len(train_dataloader) - 1:\n","                continue\n","\n","            input_tensor = input_tensor.to(device)\n","            outputs = client_model(input_tensor, labels=input_tensor)\n","            loss = outputs[0]\n","            loss.backward()\n","\n","            optimizer.step()\n","            #scheduler.step()\n","            optimizer.zero_grad()\n","            client_model.zero_grad()\n","\n","            #accumulating_batch_count += 1\n","            input_tensor = None\n","    return loss.item()"],"metadata":{"id":"UkQMTwU0f1xs","executionInfo":{"status":"ok","timestamp":1673747264899,"user_tz":300,"elapsed":336,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["def server_aggregate(global_model, client_models):\n","    \"\"\"\n","    This function has aggregation method 'mean'\n","    \"\"\"\n","    ### This will take simple mean of the weights of models ###\n","    global_dict = global_model.state_dict()\n","    for k in global_dict.keys():\n","        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n","    global_model.load_state_dict(global_dict)\n","    for model in client_models:\n","        model.load_state_dict(global_model.state_dict())"],"metadata":{"id":"_DknM_9Ik8HB","executionInfo":{"status":"ok","timestamp":1673747267062,"user_tz":300,"elapsed":1,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["############################################\n","#### Initializing models and optimizer  ####\n","############################################\n","\n","#### global model ##########\n","gen_model=GPT2LMHeadModel.from_pretrained(model_name)\n","\n","### Resume from last ckpt\n","#gen_model.load_state_dict(torch.load(ckpt_path+'global_50.pt'))\n","\n","global_model = gen_model\n","\n","############## client models ##############\n","client_models = [ gen_model for _ in range(num_selected)]\n","for model in client_models:\n","    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global model \n","\n","############### optimizers ################\n","opt = [AdamW(model.parameters(), lr=lr) for model in client_models]"],"metadata":{"id":"TmDfwJ6mqaYf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673747272452,"user_tz":300,"elapsed":2578,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}},"outputId":"72958fed-7acb-4252-b774-3e2fee908e36"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["text_raw=None"],"metadata":{"id":"gQ44txxYuJf2","executionInfo":{"status":"ok","timestamp":1673743946158,"user_tz":300,"elapsed":5,"user":{"displayName":"Md Rafi Rashid","userId":"07573253165411180386"}}},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":["## FL Training"],"metadata":{"id":"6lk6w_HoIIZN"}},{"cell_type":"code","source":["for r in range(num_rounds):\n","    # select random clients\n","    client_idx = np.random.permutation(num_clients)[:num_selected]\n","    # client update\n","    for i in tqdm(range(num_selected)):\n","        client_update(client_models[i], opt[i], train_loaders[client_idx[i]], epoch=epochs)\n","    \n","    # server aggregate\n","    server_aggregate(global_model, client_models)\n","\n","    print('after round ',r+1, 'saving global ckpt', 'global_'+str(r+1)+'.pt')\n","    \n","    torch.save(global_model.state_dict(), ckpt_path+'part_'+str(r+1)+'.pt')"],"metadata":{"id":"VyFztfjYq0by","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6fa3dbfb-fa53-4c2d-d4f2-ca29608e1e4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Training epoch 0\n","Training epoch 1\n","Training epoch 2\n","Training epoch 3\n","Training epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 1/5 [04:01<16:06, 241.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Training epoch 0\n","Training epoch 1\n","Training epoch 2\n","Training epoch 3\n","Training epoch 4\n"]}]},{"cell_type":"code","source":["#get perplexity of a text sample\n","def get_ppl(\n","    m_name,\n","    tokenizer,\n","    sample\n","):\n","    global_model.load_state_dict(torch.load(ckpt_path+m_name,map_location='cpu'))\n","    model=global_model\n","    model.eval()\n","\n","    with torch.no_grad():\n","      generated = torch.tensor(tokenizer.encode(sample)).unsqueeze(0)\n","      outputs = model(generated, labels=generated)\n","      loss= outputs[0]\n","      ppl=torch.exp(loss)\n","\n","      return ppl.item()"],"metadata":{"id":"4Si-KwMZOKHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Generating texts with seed\n","\n","def generate_text(\n","    m_name,\n","    tokenizer,\n","    prompt,\n","    entry_length=5,\n","    entry_count=1,\n","    top_p=0.8,\n","    temperature=1.,\n","):\n","    global_model.load_state_dict(torch.load(ckpt_path+m_name,map_location='cpu'))\n","    model=global_model\n","    model.eval()\n","\n","    generated_num = 0\n","    generated_list = []\n","\n","    filter_value = -float(\"Inf\")\n","\n","    with torch.no_grad():\n","\n","        for entry_idx in trange(entry_count):\n","\n","            entry_finished = False\n","\n","            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n","\n","            # Using top-p (nucleus sampling): https://github.com/huggingface/transformers/blob/master/examples/run_generation.py\n","            for i in range(entry_length):\n","                outputs = model(generated, labels=generated)\n","                loss, logits = outputs[:2]\n","                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n","\n","                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","                cumulative_probs = torch.cumsum(\n","                    F.softmax(sorted_logits, dim=-1), dim=-1\n","                )\n","\n","                sorted_indices_to_remove = cumulative_probs > top_p\n","                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n","                    ..., :-1\n","                ].clone()\n","                sorted_indices_to_remove[..., 0] = 0\n","\n","                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","                logits[:, indices_to_remove] = filter_value\n","\n","                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n","                generated = torch.cat((generated, next_token), dim=1)\n","\n","                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n","                    entry_finished = True\n","\n","                if entry_finished:\n","\n","                    generated_num = generated_num + 1\n","\n","                    output_list = list(generated.squeeze().numpy())\n","                    output_text = tokenizer.decode(output_list)\n","\n","                    generated_list.append(output_text)\n","                    break\n","            \n","            if not entry_finished:\n","                output_list = list(generated.squeeze().numpy())\n","                output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n","                generated_list.append(output_text)\n","    ppxls=[]\n","    for g in generated_list:\n","      ppxls.append(get_PPl(m_name, tokenizer, g))\n","    return generated_list, ppxls"],"metadata":{"id":"u3cIqqXw2DNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["saved_model='part_26.pt'\n","generated_texts, pp = generate_txt(saved_model, GPT2Tokenizer.from_pretrained('gpt2'),\"Gallen was born in\", 3, 20)\n","\n","for i in range(len(generated_texts)):\n","  print(generated_texts[i])\n","  print(pp[i])"],"metadata":{"id":"NE7fDL722WsP"},"execution_count":null,"outputs":[]}]}