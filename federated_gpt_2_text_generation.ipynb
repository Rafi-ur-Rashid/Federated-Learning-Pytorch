{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HcQmklTrO9gFNqX4AT7ciLAvl8zSBi8O",
      "authorship_tag": "ABX9TyMX5mrgl88zeCCGohAKmVLt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install fire"
      ],
      "metadata": {
        "id": "4SpQ8d52THL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd84efe-c696-43dc-a016-8231ea56dbf4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fire) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire) (2.2.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=88eb11f30291c31e9adef21a70016236819539e4709332d3d5f6595b0511aebe\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "WbNUkKVDTNH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fef1d8-7b39-44c6-d870-553a6607ecb7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n_5Wxx-bBFXy"
      },
      "outputs": [],
      "source": [
        "###############################\n",
        "##### importing libraries #####\n",
        "###############################\n",
        "import json \n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset   \n",
        "torch.backends.cudnn.benchmark=True\n",
        "\n",
        "import pyarrow.parquet as pq\n",
        "import pandas as pd\n",
        "import random\n",
        "import fire\n",
        "import logging\n",
        "import os\n",
        "import csv\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from statistics import mean\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Hyperparameters for federated learning #########\n",
        "num_clients = 20\n",
        "num_selected = 5\n",
        "num_rounds = 100\n",
        "epochs = 5\n",
        "batch_size = 16\n",
        "datapath='/content/drive/MyDrive/Reserach_Notebooks/data/'\n",
        "model_name=\"gpt2\"\n",
        "tokenizer_name=\"gpt2\"\n",
        "device= 'cuda'\n",
        "epochs=5\n",
        "lr=0.001\n",
        "warmup_steps=5000\n",
        "max_seq_len=128\n",
        "train_count=num_clients*1000\n",
        "output_dir=\"\"\n",
        "save_model_on_epoch=False\n",
        "ckpt_path=\"/content/drive/MyDrive/Reserach_Notebooks/ckpt/\"\n",
        "misc_path=\"/content/drive/MyDrive/Reserach_Notebooks/misc/\"\n",
        "#np.random.seed(112)\n",
        "temp=0.5\n",
        "ctrl_code=\"<|startoftext|>\""
      ],
      "metadata": {
        "id": "t9L6Q0DmJUk7"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DataComposer(torch.utils.data.Dataset):\n",
        "    \n",
        "    def __init__(self, control_code,texts, truncate=False, max_length=768):\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.texts = []\n",
        "        \n",
        "        \n",
        "        for row in texts:\n",
        "            self.texts.append(torch.tensor(\n",
        "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
        "            ))\n",
        "                \n",
        "        if truncate:\n",
        "            self.texts = self.texts[:train_count]\n",
        "        self.text_count = len(self.texts)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.text_count\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.texts[item]"
      ],
      "metadata": {
        "id": "hWphcoVyVb2e"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pack Tensors"
      ],
      "metadata": {
        "id": "DRdsVMBdY4lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None"
      ],
      "metadata": {
        "id": "nMcH2gIOVec4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read Data"
      ],
      "metadata": {
        "id": "g-R8hmRx1fTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset specific tasks\n",
        "with open(datapath+'imdb/all_texts.txt') as file:\n",
        "  all_texts=file.read()\n",
        "\n",
        "all_words=all_texts.split()\n",
        "text_raw=[]\n",
        "frm=0\n",
        "to=max_seq_len\n",
        "for i in range(train_count):\n",
        "  temp=' '.join(all_words[frm:to])\n",
        "  text_raw.append(temp)\n",
        "  frm+=max_seq_len\n",
        "  to+=max_seq_len"
      ],
      "metadata": {
        "id": "U9ZBRuotnNWS"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing the training data into num_clients, with each client having equal number of images\n",
        "def make_client_dataset(text_raw):\n",
        "\n",
        "  splits= torch.utils.data.random_split(text_raw, [int(len(text_raw) / num_clients) for _ in range(num_clients)])\n",
        "  client_data=[]\n",
        "  for s in splits:\n",
        "    temp=[]\n",
        "    for t in s:\n",
        "      temp+=[t]\n",
        "    client_data+=[temp]\n",
        "  \n",
        "  return client_data\n",
        "\n",
        "traindata_split = make_client_dataset(text_raw)"
      ],
      "metadata": {
        "id": "s8iR7heKhoZm"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a pytorch loader for a Deep Learning model\n",
        "train_loaders = [torch.utils.data.DataLoader(DataComposer(ctrl_code, x, truncate=True), batch_size=1, shuffle=True) for x in traindata_split]"
      ],
      "metadata": {
        "id": "xQAKJoevolpS"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def client_update(client_model, optimizer, train_dataloader, epoch=5):\n",
        "    \"\"\"\n",
        "    This function updates/trains client model on client data\n",
        "    \"\"\"\n",
        "    client_model = client_model.to(device)\n",
        "    client_model.train()\n",
        "\n",
        "    # scheduler = get_linear_schedule_with_warmup(\n",
        "    #     optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
        "    # )\n",
        "\n",
        "    #accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        for idx, entry in enumerate(train_dataloader):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, max_seq_len)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            outputs = client_model(input_tensor, labels=input_tensor)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            #scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            client_model.zero_grad()\n",
        "\n",
        "            #accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "UkQMTwU0f1xs"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def server_aggregate(global_model, client_models):\n",
        "    \"\"\"\n",
        "    This function has aggregation method 'mean'\n",
        "    \"\"\"\n",
        "    ### This will take simple mean of the weights of models ###\n",
        "    global_dict = global_model.state_dict()\n",
        "    for k in global_dict.keys():\n",
        "        global_dict[k] = torch.stack([client_models[i].state_dict()[k].float() for i in range(len(client_models))], 0).mean(0)\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    for model in client_models:\n",
        "        model.load_state_dict(global_model.state_dict())"
      ],
      "metadata": {
        "id": "_DknM_9Ik8HB"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################\n",
        "#### Initializing models and optimizer  ####\n",
        "############################################\n",
        "\n",
        "#### global model ##########\n",
        "gen_model=GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "### Resume from last ckpt\n",
        "#gen_model.load_state_dict(torch.load(ckpt_path+'global_50.pt'))\n",
        "\n",
        "global_model = gen_model\n",
        "\n",
        "############## client models ##############\n",
        "client_models = [ gen_model for _ in range(num_selected)]\n",
        "for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global model \n",
        "\n",
        "############### optimizers ################\n",
        "opt = [AdamW(model.parameters(), lr=lr) for model in client_models]"
      ],
      "metadata": {
        "id": "TmDfwJ6mqaYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72958fed-7acb-4252-b774-3e2fee908e36"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_raw=None"
      ],
      "metadata": {
        "id": "gQ44txxYuJf2"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FL Training"
      ],
      "metadata": {
        "id": "6lk6w_HoIIZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for r in range(num_rounds):\n",
        "    # select random clients\n",
        "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
        "    # client update\n",
        "    for i in tqdm(range(num_selected)):\n",
        "        client_update(client_models[i], opt[i], train_loaders[client_idx[i]], epoch=epochs)\n",
        "    \n",
        "    # server aggregate\n",
        "    server_aggregate(global_model, client_models)\n",
        "\n",
        "    print('after round ',r+1, 'saving global ckpt', 'global_'+str(r+1)+'.pt')\n",
        "    \n",
        "    torch.save(global_model.state_dict(), ckpt_path+'part_'+str(r+1)+'.pt')"
      ],
      "metadata": {
        "id": "VyFztfjYq0by",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa3dbfb-fa53-4c2d-d4f2-ca29608e1e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 0\n",
            "Training epoch 1\n",
            "Training epoch 2\n",
            "Training epoch 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get perplexity of a text sample\n",
        "def get_ppl(\n",
        "    m_name,\n",
        "    tokenizer,\n",
        "    sample\n",
        "):\n",
        "    global_model.load_state_dict(torch.load(ckpt_path+m_name,map_location='cpu'))\n",
        "    model=global_model\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      generated = torch.tensor(tokenizer.encode(sample)).unsqueeze(0)\n",
        "      outputs = model(generated, labels=generated)\n",
        "      loss= outputs[0]\n",
        "      ppl=torch.exp(loss)\n",
        "\n",
        "      return ppl.item()"
      ],
      "metadata": {
        "id": "4Si-KwMZOKHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Generating texts with seed\n",
        "\n",
        "def generate_text(\n",
        "    m_name,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    entry_length=5,\n",
        "    entry_count=1,\n",
        "    top_p=0.8,\n",
        "    temperature=1.,\n",
        "):\n",
        "    global_model.load_state_dict(torch.load(ckpt_path+m_name,map_location='cpu'))\n",
        "    model=global_model\n",
        "    model.eval()\n",
        "\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "\n",
        "    filter_value = -float(\"Inf\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "\n",
        "            entry_finished = False\n",
        "\n",
        "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "\n",
        "            # Using top-p (nucleus sampling): https://github.com/huggingface/transformers/blob/master/examples/run_generation.py\n",
        "            for i in range(entry_length):\n",
        "                outputs = model(generated, labels=generated)\n",
        "                loss, logits = outputs[:2]\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(\n",
        "                    F.softmax(sorted_logits, dim=-1), dim=-1\n",
        "                )\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                    ..., :-1\n",
        "                ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "\n",
        "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "                generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
        "                    entry_finished = True\n",
        "\n",
        "                if entry_finished:\n",
        "\n",
        "                    generated_num = generated_num + 1\n",
        "\n",
        "                    output_list = list(generated.squeeze().numpy())\n",
        "                    output_text = tokenizer.decode(output_list)\n",
        "\n",
        "                    generated_list.append(output_text)\n",
        "                    break\n",
        "            \n",
        "            if not entry_finished:\n",
        "                output_list = list(generated.squeeze().numpy())\n",
        "                output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
        "                generated_list.append(output_text)\n",
        "    ppxls=[]\n",
        "    for g in generated_list:\n",
        "      ppxls.append(get_PPl(m_name, tokenizer, g))\n",
        "    return generated_list, ppxls"
      ],
      "metadata": {
        "id": "u3cIqqXw2DNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model='part_26.pt'\n",
        "generated_texts, pp = generate_txt(saved_model, GPT2Tokenizer.from_pretrained('gpt2'),\"Gallen was born in\", 3, 20)\n",
        "\n",
        "for i in range(len(generated_texts)):\n",
        "  print(generated_texts[i])\n",
        "  print(pp[i])"
      ],
      "metadata": {
        "id": "NE7fDL722WsP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}